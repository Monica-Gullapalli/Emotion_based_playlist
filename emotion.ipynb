{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"/Users/monicagullapalli/Downloads/emotion_detection/train\"\n",
    "validation_folder = \"/Users/monicagullapalli/Downloads/emotion_detection/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_folder = \"//Users/monicagullapalli/Downloads/Neural-networks-assignments/Neural-Networks-project/combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Happy:   0%|          | 0/7215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Happy: 100%|██████████| 7215/7215 [00:04<00:00, 1587.76it/s]\n",
      "Copying Sad: 100%|██████████| 4830/4830 [00:03<00:00, 1481.05it/s]\n",
      "Copying Fear: 100%|██████████| 4097/4097 [00:01<00:00, 2173.94it/s]\n",
      "Copying Surprise: 100%|██████████| 3171/3171 [00:01<00:00, 2162.51it/s]\n",
      "Copying Neutral: 100%|██████████| 4965/4965 [00:01<00:00, 2522.70it/s]\n",
      "Copying Angry: 100%|██████████| 3995/3995 [00:01<00:00, 2318.17it/s]\n",
      "Copying Disgust: 100%|██████████| 436/436 [00:00<00:00, 2700.34it/s]\n",
      "Copying Happy: 100%|██████████| 879/879 [00:00<00:00, 2524.83it/s]\n",
      "Copying Sad: 100%|██████████| 594/594 [00:00<00:00, 2607.96it/s]\n",
      "Copying Fear: 100%|██████████| 528/528 [00:00<00:00, 2092.27it/s]\n",
      "Copying Surprise: 100%|██████████| 416/416 [00:00<00:00, 2205.56it/s]\n",
      "Copying Neutral: 100%|██████████| 626/626 [00:00<00:00, 2052.11it/s]\n",
      "Copying Angry: 100%|██████████| 491/491 [00:00<00:00, 2163.98it/s]\n",
      "Copying Disgust: 100%|██████████| 55/55 [00:00<00:00, 2749.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combined successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def copy_files_with_labels(source_folder, destination_folder):\n",
    "    for label in os.listdir(source_folder):\n",
    "        label_folder = os.path.join(source_folder, label)\n",
    "        if os.path.isdir(label_folder):\n",
    "            destination_label_folder = os.path.join(destination_folder, label)\n",
    "            if not os.path.exists(destination_label_folder):\n",
    "                os.makedirs(destination_label_folder)\n",
    "            for file in tqdm(os.listdir(label_folder), desc=f\"Copying {label}\"):\n",
    "                shutil.copy(os.path.join(label_folder, file), destination_label_folder)\n",
    "\n",
    "\n",
    "copy_files_with_labels(train_folder, combined_folder)\n",
    "\n",
    "\n",
    "copy_files_with_labels(validation_folder, combined_folder)\n",
    "\n",
    "print(\"Data combined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n",
      "Epoch 1/10\n",
      "898/898 [==============================] - 39s 43ms/step - loss: 1.6115 - accuracy: 0.3754 - val_loss: 1.5726 - val_accuracy: 0.3993\n",
      "Epoch 2/10\n",
      "898/898 [==============================] - 41s 45ms/step - loss: 1.4191 - accuracy: 0.4575 - val_loss: 1.5646 - val_accuracy: 0.3984\n",
      "Epoch 3/10\n",
      "898/898 [==============================] - 53s 59ms/step - loss: 1.2965 - accuracy: 0.5098 - val_loss: 1.5578 - val_accuracy: 0.4171\n",
      "Epoch 4/10\n",
      "898/898 [==============================] - 93s 103ms/step - loss: 1.1745 - accuracy: 0.5608 - val_loss: 1.6054 - val_accuracy: 0.4079\n",
      "Epoch 5/10\n",
      "898/898 [==============================] - 71s 79ms/step - loss: 1.0574 - accuracy: 0.6092 - val_loss: 1.6656 - val_accuracy: 0.4171\n",
      "Epoch 6/10\n",
      "898/898 [==============================] - 75s 84ms/step - loss: 0.9333 - accuracy: 0.6565 - val_loss: 1.7957 - val_accuracy: 0.4235\n",
      "Epoch 7/10\n",
      "898/898 [==============================] - 52s 58ms/step - loss: 0.8023 - accuracy: 0.7057 - val_loss: 1.9201 - val_accuracy: 0.4283\n",
      "Epoch 8/10\n",
      "898/898 [==============================] - 47s 52ms/step - loss: 0.6813 - accuracy: 0.7548 - val_loss: 2.0952 - val_accuracy: 0.4032\n",
      "Epoch 9/10\n",
      "898/898 [==============================] - 49s 54ms/step - loss: 0.5698 - accuracy: 0.7965 - val_loss: 2.3577 - val_accuracy: 0.4324\n",
      "Epoch 10/10\n",
      "898/898 [==============================] - 40s 44ms/step - loss: 0.4679 - accuracy: 0.8387 - val_loss: 2.5889 - val_accuracy: 0.3968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x176727f90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(7, activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    '/Users/monicagullapalli/Downloads/emotion_detection/train',\n",
    "    target_size=(64, 64), \n",
    "    batch_size=32,\n",
    "    class_mode='sparse'  \n",
    ")\n",
    "\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    '/Users/monicagullapalli/Downloads/emotion_detection/validation',\n",
    "    target_size=(64, 64), \n",
    "    batch_size=32,\n",
    "    class_mode='sparse' \n",
    ")\n",
    "\n",
    "\n",
    "model.fit(train_generator, validation_data=validation_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monicagullapalli/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('/Users/monicagullapalli/Downloads/Neural-networks-assignments/Neural-Networks-project/final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m capture_button\u001b[38;5;241m.\u001b[39mpack()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Start the Tkinter event loop\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m window\u001b[38;5;241m.\u001b[39mmainloop()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Release the camera\u001b[39;00m\n\u001b[1;32m     69\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/tkinter/__init__.py:1485\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk\u001b[38;5;241m.\u001b[39mmainloop(n)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import webbrowser\n",
    "\n",
    "# Load the pre-trained emotion classification model\n",
    "model = load_model('/Users/monicagullapalli/Downloads/Neural-networks-assignments/Neural-Networks-project/final_model.h5')\n",
    "\n",
    "class_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "# Define Spotify playlist URLs for each emotion\n",
    "spotify_playlists = {\n",
    "    'Angry': 'https://spotify.com/playlist/angry_playlist',\n",
    "    'Sad': 'https://spotify.com/playlist/sad_playlist',\n",
    "    'Happy': 'https://spotify.com/playlist/happy_playlist'\n",
    "}\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to capture image and perform actions\n",
    "def capture_image():\n",
    "    ret, frame = cap.read()\n",
    "    processed_frame = cv2.resize(frame, (64, 64))\n",
    "    processed_frame = processed_frame / 255.0\n",
    "\n",
    "    input_data = np.expand_dims(processed_frame, axis=0)\n",
    "\n",
    "    predictions = model.predict(input_data)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    predicted_label = class_labels[predicted_class]\n",
    "\n",
    "    # Perform actions based on the predicted class label\n",
    "    if predicted_label == 'Angry':\n",
    "        # Redirect to Angry playlist on Spotify\n",
    "        webbrowser.open(spotify_playlists['Angry'])\n",
    "\n",
    "    elif predicted_label == 'Sad':\n",
    "        # Redirect to Sad playlist on Spotify\n",
    "        webbrowser.open(spotify_playlists['Sad'])\n",
    "\n",
    "    elif predicted_label == 'Happy':\n",
    "        # Redirect to Happy playlist on Spotify\n",
    "        webbrowser.open(spotify_playlists['Happy'])\n",
    "\n",
    "# Create a Tkinter window\n",
    "window = tk.Tk()\n",
    "\n",
    "# Function to capture frame and perform actions\n",
    "def capture_frame():\n",
    "    # Capture a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(frame)\n",
    "\n",
    "    # Display the captured frame\n",
    "    image.show()\n",
    "\n",
    "    # Perform actions on the captured frame\n",
    "    capture_image()\n",
    "\n",
    "# Create a capture button\n",
    "capture_button = tk.Button(window, text='Capture', command=capture_frame)\n",
    "capture_button.pack()\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "window.mainloop()\n",
    "\n",
    "# Release the camera\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
