{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"/Users/monicagullapalli/Downloads/emotion_detection/train\"\n",
    "validation_folder = \"/Users/monicagullapalli/Downloads/emotion_detection/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_folder = \"//Users/monicagullapalli/Downloads/Neural-networks-assignments/Neural-Networks-project/combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Happy:   0%|          | 0/7215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Happy: 100%|██████████| 7215/7215 [00:03<00:00, 2249.80it/s]\n",
      "Copying Sad: 100%|██████████| 4830/4830 [00:02<00:00, 1969.36it/s]\n",
      "Copying Fear: 100%|██████████| 4097/4097 [00:01<00:00, 2063.82it/s]\n",
      "Copying Surprise: 100%|██████████| 3171/3171 [00:01<00:00, 2082.24it/s]\n",
      "Copying Neutral: 100%|██████████| 4965/4965 [00:02<00:00, 1928.62it/s]\n",
      "Copying Angry: 100%|██████████| 3995/3995 [00:01<00:00, 2513.77it/s]\n",
      "Copying Disgust: 100%|██████████| 436/436 [00:00<00:00, 2220.54it/s]\n",
      "Copying Happy: 100%|██████████| 879/879 [00:00<00:00, 3005.78it/s]\n",
      "Copying Sad: 100%|██████████| 594/594 [00:00<00:00, 2215.36it/s]\n",
      "Copying Fear: 100%|██████████| 528/528 [00:00<00:00, 3156.03it/s]\n",
      "Copying Surprise: 100%|██████████| 416/416 [00:00<00:00, 2850.83it/s]\n",
      "Copying Neutral: 100%|██████████| 626/626 [00:00<00:00, 2220.45it/s]\n",
      "Copying Angry: 100%|██████████| 491/491 [00:00<00:00, 3331.37it/s]\n",
      "Copying Disgust: 100%|██████████| 55/55 [00:00<00:00, 3769.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combined successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def copy_files_with_labels(source_folder, destination_folder):\n",
    "    for label in os.listdir(source_folder):\n",
    "        label_folder = os.path.join(source_folder, label)\n",
    "        if os.path.isdir(label_folder):\n",
    "            destination_label_folder = os.path.join(destination_folder, label)\n",
    "            if not os.path.exists(destination_label_folder):\n",
    "                os.makedirs(destination_label_folder)\n",
    "            for file in tqdm(os.listdir(label_folder), desc=f\"Copying {label}\"):\n",
    "                shutil.copy(os.path.join(label_folder, file), destination_label_folder)\n",
    "\n",
    "\n",
    "copy_files_with_labels(train_folder, combined_folder)\n",
    "\n",
    "\n",
    "copy_files_with_labels(validation_folder, combined_folder)\n",
    "\n",
    "print(\"Data combined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n",
      "Epoch 1/10\n",
      "898/898 [==============================] - 32s 35ms/step - loss: 1.6363 - accuracy: 0.3660 - val_loss: 1.5900 - val_accuracy: 0.3795\n",
      "Epoch 2/10\n",
      "898/898 [==============================] - 34s 38ms/step - loss: 1.4143 - accuracy: 0.4589 - val_loss: 1.5684 - val_accuracy: 0.3976\n",
      "Epoch 3/10\n",
      "898/898 [==============================] - 34s 37ms/step - loss: 1.2802 - accuracy: 0.5150 - val_loss: 1.5849 - val_accuracy: 0.4199\n",
      "Epoch 4/10\n",
      "898/898 [==============================] - 36s 40ms/step - loss: 1.1543 - accuracy: 0.5681 - val_loss: 1.6672 - val_accuracy: 0.4129\n",
      "Epoch 5/10\n",
      "898/898 [==============================] - 35s 39ms/step - loss: 1.0139 - accuracy: 0.6247 - val_loss: 1.7938 - val_accuracy: 0.4227\n",
      "Epoch 6/10\n",
      "898/898 [==============================] - 34s 38ms/step - loss: 0.8762 - accuracy: 0.6825 - val_loss: 1.9179 - val_accuracy: 0.4238\n",
      "Epoch 7/10\n",
      "898/898 [==============================] - 31s 34ms/step - loss: 0.7351 - accuracy: 0.7356 - val_loss: 2.1963 - val_accuracy: 0.4188\n",
      "Epoch 8/10\n",
      "898/898 [==============================] - 31s 34ms/step - loss: 0.6056 - accuracy: 0.7853 - val_loss: 2.4424 - val_accuracy: 0.4205\n",
      "Epoch 9/10\n",
      "898/898 [==============================] - 31s 34ms/step - loss: 0.4798 - accuracy: 0.8353 - val_loss: 2.6593 - val_accuracy: 0.4032\n",
      "Epoch 10/10\n",
      "898/898 [==============================] - 30s 33ms/step - loss: 0.3847 - accuracy: 0.8700 - val_loss: 3.0393 - val_accuracy: 0.3884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14cac8210>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(7, activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    '/Users/monicagullapalli/Downloads/emotion_detection/train',\n",
    "    target_size=(64, 64), \n",
    "    batch_size=32,\n",
    "    class_mode='sparse'  \n",
    ")\n",
    "\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    '/Users/monicagullapalli/Downloads/emotion_detection/validation',\n",
    "    target_size=(64, 64), \n",
    "    batch_size=32,\n",
    "    class_mode='sparse' \n",
    ")\n",
    "\n",
    "\n",
    "model.fit(train_generator, validation_data=validation_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monicagullapalli/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('/Users/monicagullapalli/Downloads/Neural-networks-assignments/Neural-Networks-project/final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 90.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-image file: /Users/monicagullapalli/Downloads/emotion_detection/train/.DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Function to classify all images in a folder using VGG16\n",
    "def classify_images_vgg16(folder_path):\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(image_path):\n",
    "            try:\n",
    "                Image.open(image_path)\n",
    "            except (IOError, OSError):\n",
    "                print(f\"Skipping non-image file: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Classifying image: {image_path}\")\n",
    "            classify_image_vgg16(image_path)\n",
    "\n",
    "# Provide the path to the train folder\n",
    "train_folder = \"/Users/monicagullapalli/Downloads/emotion_detection/train\"\n",
    "\n",
    "# Classify all images in the train folder using VGG16\n",
    "classify_images_vgg16(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7215/7215 [1:36:47<00:00,  1.24it/s]  \n",
      "100%|██████████| 4830/4830 [1:41:35<00:00,  1.26s/it]   \n",
      "100%|██████████| 4097/4097 [2:24:49<00:00,  2.12s/it]  \n",
      "100%|██████████| 3171/3171 [1:48:00<00:00,  2.04s/it]   \n",
      "100%|██████████| 4965/4965 [2:39:23<00:00,  1.93s/it]     \n",
      "100%|██████████| 3995/3995 [1:00:12<00:00,  1.11it/s] \n",
      "100%|██████████| 436/436 [20:12<00:00,  2.78s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0672 - Accuracy: 0.9974\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7215/7215 [2:29:07<00:00,  1.24s/it]     \n",
      "100%|██████████| 4830/4830 [1:43:55<00:00,  1.29s/it]  \n",
      "100%|██████████| 4097/4097 [2:00:48<00:00,  1.77s/it]     \n",
      " 70%|██████▉   | 2219/3171 [49:11<21:47,  1.37s/it]  "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Provide the path to the train folder\n",
    "train_folder = \"/Users/monicagullapalli/Downloads/emotion_detection/train\"\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "\n",
    "# Create the base VGG16 model\n",
    "base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model's layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add a custom output layer with 7 units for the emotion classes\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = Dense(7, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for emotion_folder in os.listdir(train_folder):\n",
    "        subfolder_path = os.path.join(train_folder, emotion_folder)\n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue\n",
    "\n",
    "        for filename in tqdm(os.listdir(subfolder_path)):\n",
    "            image_path = os.path.join(subfolder_path, filename)\n",
    "            if os.path.isfile(image_path):\n",
    "                try:\n",
    "                    Image.open(image_path)\n",
    "                except (IOError, OSError):\n",
    "                    print(f\"Skipping non-image file: {image_path}\")\n",
    "                    continue\n",
    "\n",
    "                img = image.load_img(image_path, target_size=(224, 224))\n",
    "                x = image.img_to_array(img)\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                x = preprocess_input(x)\n",
    "\n",
    "                # Assuming you have labels for each image, you can load and preprocess them here\n",
    "                emotion_classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "                label = np.zeros((1, len(emotion_classes)))\n",
    "                label[0, emotion_classes.index(emotion_folder)] = 1\n",
    "\n",
    "                # Perform training on the image batch\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = model(x, training=True)\n",
    "                    loss_value = loss_fn(label, logits)\n",
    "\n",
    "                grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "                # Update loss and accuracy\n",
    "                total_loss += loss_value\n",
    "                total_accuracy += np.mean(np.argmax(logits, axis=1) == np.argmax(label, axis=1))\n",
    "                num_batches += 1\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    if num_batches > 0:\n",
    "        average_loss = total_loss / num_batches\n",
    "        average_accuracy = total_accuracy / num_batches\n",
    "        print(f\"Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid image files found.\")\n",
    "\n",
    "    # Save the model weights after each epoch\n",
    "    model.save_weights(f\"vgg16_epoch_{epoch + 1}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import webbrowser\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Load the pre-trained emotion classification model\n",
    "model = load_model('/Users/monicagullapalli/Downloads/Neural-networks-assignments/Neural-Networks-project/final_model.h5')\n",
    "\n",
    "class_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "# Define Spotify playlist URLs for each emotion\n",
    "spotify_playlists = {\n",
    "    'Angry': 'https://spotify.com/playlist/angry_playlist',\n",
    "    'Sad': 'https://spotify.com/playlist/sad_playlist',\n",
    "    'Happy': 'https://open.spotify.com/playlist/049OGKQgNXgRRJh91zNwi7',\n",
    "    'Disgust': 'https://spotify.com/playlist/disgust_playlist',\n",
    "    'Fear': 'https://spotify.com/playlist/fear_playlist',\n",
    "    'Neutral': 'https://spotify.com/playlist/neutral_playlist',\n",
    "    'Surprise': 'https://spotify.com/playlist/surprise_playlist'\n",
    "}\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to capture image and perform actions\n",
    "def capture_image():\n",
    "    ret, frame = cap.read()\n",
    "    processed_frame = cv2.resize(frame, (64, 64))\n",
    "    processed_frame = processed_frame / 255.0\n",
    "\n",
    "    input_data = np.expand_dims(processed_frame, axis=0)\n",
    "\n",
    "    predictions = model.predict(input_data)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    predicted_label = class_labels[predicted_class]\n",
    "\n",
    "    # Perform actions based on the predicted class label\n",
    "    if predicted_label == 'Angry':\n",
    "        # Redirect to Angry playlist on Spotify\n",
    "        webbrowser.open(spotify_playlists['Angry'])\n",
    "\n",
    "    elif predicted_label == 'Sad':\n",
    "        # Redirect to Sad playlist on Spotify\n",
    "        webbrowser.open(spotify_playlists['Sad'])\n",
    "\n",
    "    elif predicted_label == 'Happy':\n",
    "        # Redirect to Happy playlist on Spotify\n",
    "        webbrowser.open(spotify_playlists['Happy'])\n",
    "\n",
    "    elif predicted_label == 'Disgust':\n",
    "        # Redirect to Disgust playlist on Spotify\n",
    "        webbrowser.open(spotify_playlists['Disgust'])\n",
    "\n",
    "    elif predicted_label == 'Fear':\n",
    "        # Redirect to Fear playlist on Spotify\n",
    "        webbrowser.open(spotify_playlists['Fear'])\n",
    "\n",
    "    elif predicted_label == 'Neutral':\n",
    "        # Redirect to Neutral playlist on Spotify\n",
    "        webbrowser.open(spotify_playlists['Neutral'])\n",
    "\n",
    "    elif predicted_label == 'Surprise':\n",
    "        # Redirect to Surprise playlist on Spotify\n",
    "        webbrowser.open(spotify_playlists['Surprise'])\n",
    "\n",
    "# Create a Tkinter window\n",
    "window = tk.Tk()\n",
    "\n",
    "# Function to capture frame and perform actions\n",
    "def capture_frame():\n",
    "    # Capture a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(frame)\n",
    "\n",
    "    # Display the captured frame\n",
    "    image.show()\n",
    "\n",
    "    # Perform actions on the captured frame\n",
    "    capture_image()\n",
    "\n",
    "# Create a capture button\n",
    "capture_button = tk.Button(window, text='Capture', command=capture_frame)\n",
    "capture_button.pack()\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "window.mainloop()\n",
    "\n",
    "# Release the camera\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
